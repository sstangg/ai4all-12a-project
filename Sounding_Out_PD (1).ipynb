{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Improved Parkinson's Disease Detection Model\n",
        "XGBoost + 20 Features + SMOTE+ENN with Proper Aggregation\n",
        "Eliminates data leakage through speaker-aware aggregation and splitting\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, classification_report, confusion_matrix, mean_squared_error\n",
        ")\n",
        "from collections import Counter\n",
        "from imblearn.combine import SMOTEENN\n",
        "import shap\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"dipayanbiswas/parkinsons-disease-speech-signal-features\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "def load_and_aggregate_data(file_path):\n",
        "    \"\"\"Load and aggregate data by speaker ID to prevent data leakage\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "    print(f\"Number of unique speakers: {df['id'].nunique()}\")\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "\n",
        "    # Aggregate by speaker ID - one row per speaker\n",
        "    # Group by 'id' and calculate mean for all numeric features\n",
        "    feature_cols = [col for col in df.columns if col not in ['id', 'class']]\n",
        "\n",
        "    # Create aggregation dictionary - mean for all features\n",
        "    agg_dict = {col: 'mean' for col in feature_cols}\n",
        "    # For class, take the mode (most common value) or max (assuming 1 = PD)\n",
        "    agg_dict['class'] = 'max'  # If any sample is PD, speaker is PD\n",
        "\n",
        "    aggregated_df = df.groupby('id').agg(agg_dict).reset_index()\n",
        "\n",
        "    print(f\"\\nAggregated dataset shape: {aggregated_df.shape}\")\n",
        "    print(f\"Class distribution after aggregation: {Counter(aggregated_df['class'])}\")\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "def get_top_20_features(df, exclude_cols=['id', 'class']):\n",
        "    \"\"\"Get top 20 features based on absolute correlation with target\"\"\"\n",
        "    # Work with feature columns only\n",
        "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "    # Calculate correlations with class\n",
        "    correlations = pd.Series(\n",
        "        {col: abs(df[col].corr(df['class'])) for col in feature_cols}\n",
        "    ).sort_values(ascending=False)\n",
        "\n",
        "    # Select top 20 features\n",
        "    top_20_features = correlations.head(20).index.tolist()\n",
        "\n",
        "    print(f\"\\nSelected top 20 features:\")\n",
        "    for i, feature in enumerate(top_20_features, 1):\n",
        "        corr_val = df[feature].corr(df['class'])\n",
        "        print(f\"{i:2d}. {feature[:30]:30} (correlation: {corr_val:6.4f})\")\n",
        "\n",
        "    return top_20_features\n",
        "\n",
        "def group_aware_train_test_split(df, test_size=0.25):\n",
        "    \"\"\"\n",
        "    Perform train-test split ensuring no speaker appears in both sets\n",
        "    Stratified by class to maintain class balance\n",
        "    \"\"\"\n",
        "    # Separate features, target, and groups\n",
        "    X = df.drop(['class', 'id'], axis=1)\n",
        "    y = df['class']\n",
        "    groups = df['id']\n",
        "\n",
        "    # Use GroupShuffleSplit to ensure no ID appears in both train and test\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=RANDOM_STATE)\n",
        "\n",
        "    for train_idx, test_idx in gss.split(X, y, groups):\n",
        "        X_train_initial = X.iloc[train_idx]\n",
        "        X_test = X.iloc[test_idx]\n",
        "        y_train_initial = y.iloc[train_idx]\n",
        "        y_test = y.iloc[test_idx]\n",
        "        train_ids = groups.iloc[train_idx]\n",
        "        test_ids = groups.iloc[test_idx]\n",
        "\n",
        "    # Verify no overlap\n",
        "    train_id_set = set(train_ids)\n",
        "    test_id_set = set(test_ids)\n",
        "    assert len(train_id_set.intersection(test_id_set)) == 0, \"Data leakage detected!\"\n",
        "\n",
        "    print(f\"\\nTrain set: {len(X_train_initial)} speakers\")\n",
        "    print(f\"Test set: {len(X_test)} speakers\")\n",
        "    print(f\"Train class distribution: {Counter(y_train_initial)}\")\n",
        "    print(f\"Test class distribution: {Counter(y_test)}\")\n",
        "\n",
        "    return X_train_initial, X_test, y_train_initial, y_test\n",
        "\n",
        "def apply_smote_enn_to_train(X_train, y_train):\n",
        "    \"\"\"Apply SMOTE + ENN oversampling only to training data\"\"\"\n",
        "    print(f\"\\nOriginal training class distribution: {Counter(y_train)}\")\n",
        "\n",
        "    smote_enn = SMOTEENN(random_state=RANDOM_STATE)\n",
        "    X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(f\"After SMOTE + ENN on training: {Counter(y_train_resampled)}\")\n",
        "\n",
        "    return X_train_resampled, y_train_resampled\n",
        "\n",
        "def create_best_model():\n",
        "    \"\"\"Create XGBoost model with optimal hyperparameters\"\"\"\n",
        "    return xgb.XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        scale_pos_weight=1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        eval_metric='logloss',\n",
        "        objective='binary:logistic'\n",
        "    )\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision (0)': precision_score(y_test, y_pred, pos_label=0),\n",
        "        'Precision (1)': precision_score(y_test, y_pred, pos_label=1),\n",
        "        'Recall (0)': recall_score(y_test, y_pred, pos_label=0),\n",
        "        'Recall (1)': recall_score(y_test, y_pred, pos_label=1),\n",
        "        'F1 score (0)': f1_score(y_test, y_pred, pos_label=0),\n",
        "        'F1 score (1)': f1_score(y_test, y_pred, pos_label=1),\n",
        "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba),\n",
        "        'MSE': mean_squared_error(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    return metrics, y_pred\n",
        "\n",
        "def train_best_model(file_path):\n",
        "    \"\"\"Main training pipeline with proper aggregation and no data leakage\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"IMPROVED PARKINSON'S DISEASE DETECTION MODEL\")\n",
        "    print(\"XGBoost + 20 Features + SMOTE+ENN (No Data Leakage)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Load and aggregate data by speaker ID\n",
        "    aggregated_df = load_and_aggregate_data(file_path)\n",
        "\n",
        "    # 2. Feature Selection - Top 20 correlation-based features\n",
        "    top_20_features = get_top_20_features(aggregated_df)\n",
        "\n",
        "    # 3. Prepare data with selected features\n",
        "    X_selected = aggregated_df[top_20_features]\n",
        "    y = aggregated_df['class']\n",
        "\n",
        "    # Create temporary dataframe for splitting (includes ID for group-aware split)\n",
        "    split_df = pd.concat([\n",
        "        aggregated_df[['id']],\n",
        "        X_selected,\n",
        "        y\n",
        "    ], axis=1)\n",
        "\n",
        "    # 4. Group-aware train/test split (no speaker in both sets)\n",
        "    X_train, X_test, y_train, y_test = group_aware_train_test_split(split_df)\n",
        "\n",
        "    # 5. Apply SMOTE + ENN only to training data\n",
        "    X_train_resampled, y_train_resampled = apply_smote_enn_to_train(X_train, y_train)\n",
        "\n",
        "    # 6. Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "    X_test_scaled = scaler.transform(X_test)  # No synthetic data in test set\n",
        "\n",
        "    # 7. Create and train model\n",
        "    model = create_best_model()\n",
        "    print(f\"\\nTraining XGBoost model...\")\n",
        "    model.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "    # 8. Evaluate model on real test data (no synthetic samples)\n",
        "    metrics, y_pred = evaluate_model(model, X_test_scaled, y_test)\n",
        "\n",
        "    # 9. Display results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL MODEL PERFORMANCE (on real test data)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\nTest Set Results:\")\n",
        "    print(\"-\" * 30)\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric:15}: {value:.4f}\")\n",
        "\n",
        "    # 10. Confusion Matrix\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "    print(f\"True Negatives: {cm[0,0]}, False Positives: {cm[0,1]}\")\n",
        "    print(f\"False Negatives: {cm[1,0]}, True Positives: {cm[1,1]}\")\n",
        "\n",
        "    # 11. Classification Report\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Non-PD', 'PD']))\n",
        "\n",
        "    # 12. Feature Importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': top_20_features,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"\\nTop 10 Most Important Features:\")\n",
        "    print(\"-\" * 40)\n",
        "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
        "        print(f\"{i:2d}. {row['feature'][:25]:25} ({row['importance']:.4f})\")\n",
        "\n",
        "\n",
        "    # 13. compute the SHAP values for the linear model\n",
        "    background_adult = shap.maskers.Independent(X_test_scaled, max_samples=100)\n",
        "\n",
        "    # compute SHAP values\n",
        "    explainer = shap.Explainer(model, background_adult)\n",
        "    shap_values = explainer(X_test_scaled)\n",
        "    shap_values.feature_names = top_20_features\n",
        "\n",
        "    # Now, when you plot, the feature names from 'top_20' will be used\n",
        "    shap.plots.bar(shap_values, max_display=len(top_20_features))\n",
        "\n",
        "    print(\"\\nâœ… MODEL TRAINING COMPLETE (No Data Leakage!)\")\n",
        "    print(f\"Model trained on aggregated speaker data\")\n",
        "    print(f\"Test set contains no speakers from training set\")\n",
        "    print(f\"SMOTE+ENN applied only to training data\")\n",
        "\n",
        "    return model, scaler, top_20_features, metrics\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your data (update path as needed)\n",
        "    file_path = \"/kaggle/input/parkinsons-disease-speech-signal-features/pd_speech_features.csv\"\n",
        "\n",
        "    try:\n",
        "        # Train the best model with proper data handling\n",
        "        model, scaler, features, metrics = train_best_model(file_path)\n",
        "\n",
        "        # Summary of configuration\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"MODEL CONFIGURATION SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Algorithm: XGBoost\")\n",
        "        print(f\"Features: 20 (correlation-based selection)\")\n",
        "        print(f\"Data Handling: Aggregated by speaker ID\")\n",
        "        print(f\"Train-Test Split: Group-aware (no speaker overlap)\")\n",
        "        print(f\"Oversampling: SMOTE + ENN (training only)\")\n",
        "        print(f\"Test Data: Real samples only (no synthetic)\")\n",
        "        print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
        "        print(f\"Recall (PD): {metrics['Recall (1)']:.4f}\")\n",
        "        print(f\"ROC-AUC: {metrics['ROC-AUC']:.4f}\")\n",
        "        print(f\"MSE: {metrics['MSE']:.4f}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Please update the file_path variable to point to your dataset\")\n",
        "        print(\"Current path:\", file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPLKl-GSnqgZ",
        "outputId": "063d0863-3af0-4e6f-d7e4-a092cddc09de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "IMPROVED PARKINSON'S DISEASE DETECTION MODEL\n",
            "XGBoost + 20 Features + SMOTE+ENN (No Data Leakage)\n",
            "================================================================================\n",
            "Please update the file_path variable to point to your dataset\n",
            "Current path: /kaggle/input/parkinsons-disease-speech-signal-features/pd_speech_features.csv\n"
          ]
        }
      ]
    }
  ]
}